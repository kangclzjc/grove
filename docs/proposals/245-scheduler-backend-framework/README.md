# GREP-245: Scheduler Backend Framework

<!-- toc -->
- [Summary](#summary)
- [Motivation](#motivation)
  - [Goals](#goals)
  - [Non-Goals](#non-goals)
- [Proposal](#proposal)
  - [User Stories (<em>Optional</em>)](#user-stories-optional)
    - [Story 1 (<em>Optional</em>)](#story-1-optional)
    - [Story 2 (<em>Optional</em>)](#story-2-optional)
  - [Limitations/Risks &amp; Mitigations](#limitationsrisks--mitigations)
- [Design Details](#design-details)
  - [Monitoring](#monitoring)
  - [Dependencies (<em>Optional</em>)](#dependencies-optional)
  - [Test Plan](#test-plan)
  - [Graduation Criteria](#graduation-criteria)
- [Implementation History (<em>Optional</em>)](#implementation-history-optional)
- [Alternatives (<em>Optional</em>)](#alternatives-optional)
- [Appendix (<em>Optional</em>)](#appendix-optional)
<!-- /toc -->

<!--
Include a table of contents as it helps to navigate easily in the document.

Ensure the TOC is wrapped with
   <code>&lt;!-- toc --&gt;&lt;!-- /toc --&gt;</code>
tags, and then generate by invoking the make target `update-toc`.

-->

## Summary

Grove currently only supports the KAI scheduler. As more customers and schedulers begin to adopt Grove, it is essential to make Grove more extensible and easier to integrate with multiple scheduler backends. This proposal introduces a Scheduler Backend Framework that standardizes and simplifies the process of adding new scheduler support to Grove, minimizing invasive modifications required for future scheduler integrations. The framework provides a clear abstraction layer between Grove's control plane and various scheduler implementations, enabling seamless scheduler backend extensibility while maintaining backward compatibility with existing KAI scheduler deployments.

## Motivation

Many scenarios and customers today use different schedulers, including the Kubernetes default scheduler and schedulers built on the scheduler plugin framework. These schedulers require appropriate support from Grove, especially as the Kubernetes community continues to improve AI workload scheduling capabilities, including gang scheduling and topology-aware scheduling (TAS). Even the KAI scheduler required significant modifications to support Grove's PodGang API. 

The current tight coupling between Grove and specific scheduler implementations creates several challenges:

* **High Integration Cost**: Adding support for a new scheduler requires extensive modifications across Grove's codebase, touching multiple components and requiring deep knowledge of both Grove and the target scheduler's internals.
* **Maintenance Burden**: Each scheduler integration introduces scheduler-specific code paths that must be maintained independently, increasing complexity and the risk of regressions.
* **Limited Extensibility**: The lack of a standardized interface makes it difficult for third-party scheduler developers to integrate with Grove without modifying Grove's core code.
* **Scheduler Vendor Lock-in**: Users who want to switch schedulers face significant migration challenges due to scheduler-specific implementations.

Introducing a Scheduler Backend Framework addresses these challenges by providing:

* **Standardized Integration**: A well-defined interface and abstraction layer that makes adding a new scheduler backend easier and more standardized.
* **Reduced Invasiveness**: Minimal modifications to existing scheduler implementations, allowing schedulers to integrate with Grove through a plugin-like architecture.
* **Improved Scheduling Flow**: A more efficient and streamlined scheduling workflow that clearly separates concerns between Grove's workload management and scheduler-specific placement decisions.
* **Future-Proof Architecture**: A foundation that can adapt to emerging scheduling requirements and new scheduler implementations in the Kubernetes ecosystem.

In summary, refining Grove and introducing a Scheduler Backend Framework is both an urgent and inevitable improvement that will ensure Grove's long-term viability and extensibility in the evolving Kubernetes scheduling landscape.

### Goals

* **Define Scheduler Backend Interface**: Introduce a well-defined Go interface that abstracts scheduler-specific operations, enabling Grove to work with multiple scheduler backends through a standardized contract.
* **Refine PodGang Lifecycle Management**: Optimize the PodGang creation and update workflow to integrate with the Scheduler Backend Framework, allowing scheduler backends to customize pod specifications during reconciliation.
* **Enable Custom Resource Management**: Provide interfaces that allow Scheduler Backends to create, update, and delete their own custom resources in response to PodGang lifecycle events (create, update, delete, status changes).
* **Simplify User Experience**: Allow users to configure their preferred scheduler backend during Grove installation via OperatorConfiguration, eliminating the need to specify schedulerName in every pod specification.
* **Maintain Backward Compatibility**: Ensure existing KAI scheduler deployments continue to work without any changes.
* **Support Dynamic Backend Selection**: Enable Grove to determine which scheduler backend to use based on configuration, with clear mechanisms for backend registration and initialization.

### Non-Goals

* **Implement Additional Scheduler Backends**: Adding support for the default Kubernetes scheduler or other schedulers beyond KAI is explicitly out of scope for this initial framework implementation. The community support for gang scheduling and topology-aware scheduling in the default scheduler is still evolving. Support for additional schedulers will be added in subsequent PRs once the framework is established.
* **Change PodGang API Semantics**: PodGang remains Grove's primary scheduler API. This proposal does not alter the fundamental contract or semantics of the PodGang API, only how it interfaces with scheduler backends.
* **Remove Existing KAI Scheduler Modifications**: The current modifications made to KAI scheduler to support PodGang will remain. This proposal does not aim to revert or remove those changes, but rather to provide a framework that could reduce such modifications for future schedulers.
* **Extract PodGang Reconciler**: Moving the PodGang reconciliation logic from the PodCliqueSet reconciler into an independent reconciler is out of scope. The current reconciliation architecture will be maintained.
* **Multi-Scheduler Support**: Running multiple different schedulers simultaneously within a single Grove installation is not supported in this iteration. Users can only configure one scheduler backend per Grove deployment.
* **Scheduler Performance Optimization**: This proposal focuses on extensibility and maintainability, not on optimizing the performance characteristics of any particular scheduler implementation.


## Proposal

The Scheduler Backend Framework introduces a plugin-like architecture that decouples Grove's workload management from scheduler-specific implementations. The framework consists of three main components:

1. **Backend Interface**: A Go interface defining the contract between Grove and scheduler backends
2. **Backend Registry**: A registration mechanism allowing backends to register themselves during initialization
3. **Backend Lifecycle Hooks**: Well-defined points in the PodGang lifecycle where backends can inject custom logic

The framework follows a provider pattern where:
- Grove manages the high-level workflow and PodGang lifecycle
- Scheduler backends implement the interface to provide scheduler-specific behavior
- The operator configuration determines which backend is active at runtime

### User Stories

#### Story 1: Third-Party Scheduler Integration

As a third-party scheduler developer, I want to integrate my custom gang scheduler with Grove without modifying Grove's core codebase. The Scheduler Backend Framework should provide clear interfaces and documentation that allow me to implement a backend plugin for my scheduler, register it with Grove, and have Grove automatically use my scheduler for workload placement decisions.

#### Story 2: Multi-Cluster Deployment with Different Schedulers

As a platform engineer managing multiple Kubernetes clusters, I want to deploy Grove across clusters that use different schedulers (e.g., KAI in production clusters, default scheduler in development clusters). The framework should allow me to configure the appropriate scheduler backend for each cluster through OperatorConfiguration without changing workload specifications or Grove's deployment manifests.

#### Story 3: Scheduler Migration Path

As a cluster administrator, I want to migrate from one scheduler to another (e.g., from a custom scheduler to KAI or vice versa) without significant disruption. The Scheduler Backend Framework should provide a clear migration path where I can update the OperatorConfiguration, restart Grove, and have new workloads use the new scheduler while existing workloads continue running.

### Limitations/Risks & Mitigations

#### Single Backend Per Deployment

**Limitation**: Grove can only be configured with one scheduler backend per deployment. Users cannot mix schedulers for different workloads within the same Grove installation.

**Mitigation**: This is acceptable for most use cases as clusters typically standardize on a single scheduler. Users requiring multiple schedulers can run separate Grove installations with different configurations in different namespaces or clusters.

#### Backend Implementation Responsibility

**Risk**: Scheduler backend implementations may introduce bugs or incompatibilities that affect Grove's functionality.

**Mitigation**: 
- Provide comprehensive interface documentation and reference implementations
- Define clear contracts and invariants that backends must maintain
- Implement validation in Grove to detect and reject invalid backend behaviors
- Provide a testing framework for backend developers to validate their implementations

#### Migration Complexity

**Risk**: Migrating between scheduler backends may require workload redeployment and could cause temporary disruption.

**Mitigation**: 
- Document clear migration procedures and best practices
- Provide validation tools to test backend configurations before applying them
- Recommend blue-green deployment strategies for production migrations

#### Backend API Stability

**Risk**: Changes to the backend interface in future Grove versions could break existing backend implementations.

**Mitigation**:
- Follow semantic versioning for backend interfaces
- Maintain backward compatibility within major versions
- Provide deprecation notices and migration guides for interface changes
- Consider versioned interfaces if breaking changes are necessary

## Design Details

### Architecture Overview

The Scheduler Backend Framework introduces a clean separation between Grove's control plane logic and scheduler-specific implementations. The architecture consists of the following components:

<img src="assets/scheduler-backend-architecture.excalidraw.png" alt="scheduler-backend-architecture" style="zoom:50%;" />

**Key Data Flow:**

1. **PodCliqueSet Controller** → Creates PodGang (empty PodReferences, Initialized=False)
2. **Backend Controller** → Watches PodGang → Calls `backend.SyncPodGang()` → Creates scheduler CRs
3. **PodClique Controller** → Creates Pods → Calls `backend.PreparePod()` → Sets schedulerName
4. **PodCliqueSet Controller** → Fills PodReferences → Sets Initialized=True
5. **PodClique Controller** → Observes Initialized=True → Removes scheduling gates
6. **Scheduler** → Schedules pods

### Backend Interface Definition

The core of the framework is the `SchedulerBackend` interface, which defines all operations that a scheduler backend must implement. The interface is intentionally simple and focused:

```go

// SchedulerBackend defines the interface that different scheduler backends must implement.
//
// Architecture: Backend converts PodGang to scheduler-specific CR (PodGroup/Workload/etc)
// and prepares Pods with scheduler-specific configurations.
type SchedulerBackend interface {
	// Name is a unique name of the scheduler backend.
	// Used for logging and identification purposes.
	Name() string

	// Init provides a hook to initialize/setup one-time scheduler resources,
	// called at the startup of Grove operator.
	// Backends can perform tasks such as:
	// - Creating global custom resources required by the scheduler
	// - Validating scheduler availability and configuration
	// - Setting up any initial state
	Init() error

	// SyncPodGang synchronizes (creates/updates) scheduler specific resources for a PodGang
	// reacting to a creation or update of a PodGang resource.
	// This is called by the Backend Controller when PodGang spec changes.
	// Backends should:
	// - Create scheduler-specific custom resources (e.g., PodGroup, Workload)
	// - Update existing resources if the PodGang spec changed
	// - Use owner references to enable automatic cleanup
	SyncPodGang(ctx context.Context, podGang *groveschedulerv1alpha1.PodGang) error

	// OnPodGangDelete cleans up scheduler specific resources for the given PodGang.
	// This is called when a PodGang is deleted.
	// Note: If using owner references, cleanup is automatic and this can be a no-op.
	OnPodGangDelete(ctx context.Context, podGang *groveschedulerv1alpha1.PodGang) error

	// PreparePod adds scheduler backend specific configuration to the given Pod object
	// prior to its creation. This includes setting schedulerName,
	// annotations, etc.
	// This is called during Pod creation in the PodClique controller.
	PreparePod(pod *corev1.Pod)
}
```

**Key Design Decisions:**

1. **Simple Interface**: Only 5 methods, focusing on essential operations
2. **No Context in PreparePod**: Pod preparation is synchronous and doesn't need async operations
3. **Single PodGang Hook**: `SyncPodGang` handles both create and update, simplifying the interface

### Backend Manager

The manager handles backend initialization and provides global access to the active backend instance using a singleton pattern:

```go

// Initialize creates the global backend instance based on schedulerName.
// This should be called once during operator startup before controllers start.
// Supported scheduler names: "kai-scheduler", "default-scheduler"
func Initialize(client client.Client, scheme *runtime.Scheme, eventRecorder record.EventRecorder, schedulerName string) error {
	var initErr error
	initOnce.Do(func() {
		// Default to "default-scheduler" if not specified
		if schedulerName == "" {
			schedulerName = "default-scheduler"
		}

		// Create the appropriate backend based on scheduler name
		switch schedulerName {
		case "kai-scheduler":
			globalBackend = kai.New(client, scheme, eventRecorder, schedulerName)

		case "default-scheduler":
			globalBackend = kube.New(client, scheme, eventRecorder, schedulerName)

		// Future backends - uncomment and implement as needed:
		// case "volcano":
		//     globalBackend = volcano.New(client, scheme, eventRecorder, schedulerName)

		default:
			initErr = fmt.Errorf("unsupported scheduler %q (supported: kai-scheduler, default-scheduler)", schedulerName)
			return
		}

		globalSchedulerName = schedulerName

		// Initialize the backend
		if err := globalBackend.Init(); err != nil {
			initErr = fmt.Errorf("failed to initialize backend: %w", err)
			globalBackend = nil
			return
		}
	})
	return initErr
}

// Get returns the global backend instance.
// Returns nil if not initialized (caller should check).
func Get() SchedulerBackend {
	return globalBackend
}

// PreparePod is a convenience function that calls PreparePod on the global backend.
func PreparePod(pod *corev1.Pod) error {
	backend := Get()
	if backend == nil {
		return fmt.Errorf("backend not initialized")
	}
	backend.PreparePod(pod)
	return nil
}
```

**Design Rationale:**

- **Singleton Pattern**: Simpler than a full registry for the single-backend-per-cluster use case
- **Switch-Case Selection**: Clear and explicit backend selection, easy to add new backends
- **Initialization Safety**: `sync.Once` ensures thread-safe single initialization
- **Explicit Backend Imports**: All backends are compiled in, making dependencies clear

### OperatorConfiguration Extension

The OperatorConfiguration is extended with a simple `schedulerName` field to select the backend:

```go
// OperatorConfiguration defines the configuration for the Grove operator.
type OperatorConfiguration struct {
	// ... existing fields (Controllers, LogLevel, etc.) ...
	
	// SchedulerName is the name of the scheduler backend with which this instance of Grove operator will run.
	// Valid values: "kai-scheduler" or "default-scheduler"
	// Defaults to "default-scheduler" if not specified.
	// +optional
	SchedulerName string `json:"schedulerName,omitempty"`
	
	// TopologyAwareScheduling configures TAS (existing field)
	TopologyAwareScheduling TopologyAwareSchedulingConfiguration `json:"topologyAwareScheduling"`
	
	// ... other existing fields ...
}
```

Example OperatorConfiguration YAML:

```yaml
apiVersion: operator.config.grove.io/v1alpha1
kind: OperatorConfiguration
metadata:
  name: grove-config
controllers:
  podCliqueSet:
    concurrentSyncs: 3
  podClique:
    concurrentSyncs: 3
# Select the scheduler backend - "kai-scheduler" or "default-scheduler"
schedulerName: "kai-scheduler"
logLevel: info
logFormat: json
topologyAwareScheduling:
  enabled: true
  levels:
    - domain: zone
      key: "topology.kubernetes.io/zone"
    - domain: rack
      key: "topology.kubernetes.io/rack"
    - domain: host
      key: "kubernetes.io/hostname"
```

**Helm Chart Configuration:**

The Helm chart `values.yaml` exposes the scheduler configuration:

```yaml
config:
  controllers:
    podCliqueSet:
      concurrentSyncs: 3
    podClique:
      concurrentSyncs: 3
  # SchedulerName is the name of the scheduler backend
  # Valid values: "kai-scheduler" or "default-scheduler"
  # Default: "default-scheduler"
  schedulerName: ""
  logLevel: info
  logFormat: json
  topologyAwareScheduling:
    enabled: false
```

### KAI Backend Implementation

The KAI scheduler backend serves as the reference implementation:

```go
// Name returns the backend name.
func (b *Backend) Name() string {
	return BackendName
}

// Init initializes the KAI backend.
// For KAI backend, no special initialization is needed currently.
func (b *Backend) Init() error {
	return nil
}

// SyncPodGang converts PodGang to KAI PodGroup and synchronizes it.
// TODO: Currently disabled - will be implemented in phase 2.
// Phase 1: KAI scheduler still reads PodGang directly (existing behavior).
// Phase 2: Will convert PodGang to PodGroup (scheduling.run.ai/v2alpha2) for cleaner separation.
func (b *Backend) SyncPodGang(_ context.Context, _ *groveschedulerv1alpha1.PodGang) error {
	// Phase 1: Skip PodGroup creation/update
	// Phase 2: Will convert PodGang to PodGroup and synchronize
	return nil
}

// OnPodGangDelete removes the PodGroup owned by this PodGang.
// TODO: Currently disabled - will be implemented in phase 2.
func (b *Backend) OnPodGangDelete(_ context.Context, _ *groveschedulerv1alpha1.PodGang) error {
	// Phase 1: Skip PodGroup deletion
	// Phase 2: Will delete PodGroup when PodGang is deleted
	return nil
}

// PreparePod adds KAI scheduler-specific configuration to the Pod.
// This includes: schedulerName and annotations for observability.
func (b *Backend) PreparePod(pod *corev1.Pod) {
	// Set scheduler name from configuration
	pod.Spec.SchedulerName = b.schedulerName

	// Add annotations for observability
	if pod.Annotations == nil {
		pod.Annotations = make(map[string]string)
	}

	// Get PodGang and PodClique names from labels
	if podGangName, ok := pod.Labels[common.LabelPodGang]; ok {
		pod.Annotations["kai.scheduler/podgang"] = podGangName
	}
	if podCliqueName, ok := pod.Labels[common.LabelPodClique]; ok {
		pod.Annotations["kai.scheduler/podgroup"] = podCliqueName
	}
}
```

**Phase 1 vs Phase 2:**

- **Phase 1** (Current): KAI backend only implements `PreparePod`. KAI scheduler continues to read PodGang CRs directly.
- **Phase 2** (Future): KAI backend will implement `SyncPodGang` to create PodGroup CRs (scheduling.run.ai/v2alpha2), providing cleaner separation and allowing KAI scheduler modifications to be minimized.

### Kube Backend Implementation (Default Scheduler)

The Kube backend is a minimal implementation for the Kubernetes default scheduler:

```go
// Name returns the backend name.
func (b *Backend) Name() string {
	return BackendName
}

// Init initializes the Kube backend.
// For Kube backend, no special initialization is needed.
func (b *Backend) Init() error {
	return nil
}

// SyncPodGang synchronizes PodGang resources.
// For default kube scheduler, no additional resources are needed.
// Future: May create Workload CRs when KEP-4817 support is added.
func (b *Backend) SyncPodGang(_ context.Context, _ *groveschedulerv1alpha1.PodGang) error {
	// No-op: default kube scheduler doesn't need any custom resources yet
	// Future: Will create Workload CRs for gang scheduling support
	return nil
}

// OnPodGangDelete handles PodGang deletion.
// For default kube scheduler, no cleanup is needed.
func (b *Backend) OnPodGangDelete(_ context.Context, _ *groveschedulerv1alpha1.PodGang) error {
	// No-op: default kube scheduler doesn't have any resources to clean up
	return nil
}

// PreparePod adds Kubernetes default scheduler-specific configuration to the Pod.
// This simply sets the scheduler name.
func (b *Backend) PreparePod(pod *corev1.Pod) {
	// Set scheduler name from configuration
	pod.Spec.SchedulerName = b.schedulerName
}
```

### PodGang Lifecycle Changes

**Critical Design Change**: The PodGang creation flow has been fundamentally changed to support scheduler backends that require resources to exist before pods are created (e.g., Workload API):

#### Previous Flow (Before Framework):
1. Pods are created first
2. Wait for all pods to have back-references to PodGang
3. Create PodGang with complete PodReferences

#### New Flow (With Framework):
1. **Create PodGang early** with PodGroups having empty PodReferences and `Initialized=False`
2. **Backend Controller reacts** to PodGang creation and calls `SyncPodGang` to create scheduler-specific resources
3. **Pods are created** (with scheduling gates blocking actual scheduling)
4. **Operator fills PodReferences** once all pods exist, and sets `Initialized=True`
5. **PodClique Controller observes** `Initialized=True` and removes scheduling gates to allow scheduling

#### New PodGang Status Condition

```go
const (
	// PodGangConditionTypeInitialized indicates that the PodGang has been populated
	// with pod references and pods can lift scheduling gates.
	PodGangConditionTypeInitialized PodGangConditionType = "Initialized"
)
```

This condition signals that:
- All expected pods have been created
- PodGang.Spec.PodGroups[].PodReferences have been populated
- Pods can now lift their scheduling gates and proceed with scheduling

### Backend Controller

A dedicated Backend Controller watches PodGang resources and invokes backend hooks:

```go
// BackendReconciler reconciles PodGang objects and converts them to scheduler-specific CRs.
type BackendReconciler struct {
	client.Client
	Scheme  *runtime.Scheme
	Backend schedulerbackend.SchedulerBackend
}

// Reconcile processes PodGang changes and synchronizes to backend-specific CRs.
func (r *BackendReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
	// Fetch the PodGang
	podGang := &groveschedulerv1alpha1.PodGang{}
	if err := r.Get(ctx, req.NamespacedName, podGang); err != nil {
		return ctrl.Result{}, client.IgnoreNotFound(err)
	}

	// Handle deletion
	if !podGang.DeletionTimestamp.IsZero() {
		if err := r.Backend.OnPodGangDelete(ctx, podGang); err != nil {
			return ctrl.Result{}, err
		}
		return ctrl.Result{}, nil
	}

	// Sync PodGang to backend-specific CR
	if err := r.Backend.SyncPodGang(ctx, podGang); err != nil {
		return ctrl.Result{}, err
	}

	return ctrl.Result{}, nil
}

// SetupWithManager sets up the controller with the Manager.
func (r *BackendReconciler) SetupWithManager(mgr ctrl.Manager) error {
	return ctrl.NewControllerManagedBy(mgr).
		For(&groveschedulerv1alpha1.PodGang{}).
		WithEventFilter(podGangSpecChangePredicate()).
		Named(fmt.Sprintf("backend-%s", r.Backend.Name())).
		Complete(r)
}

// podGangSpecChangePredicate filters PodGang events to only process spec changes.
// Status-only updates (like Initialized condition) are ignored.
func podGangSpecChangePredicate() predicate.Predicate {
	return predicate.Funcs{
		CreateFunc: func(_ event.CreateEvent) bool {
			return true // Always process creation events
		},
		DeleteFunc: func(_ event.DeleteEvent) bool {
			return true // Process deletion for cleanup
		},
		UpdateFunc: func(e event.UpdateEvent) bool {
			// Only process if generation changed (spec was modified)
			// Generation doesn't change for status-only updates
			return e.ObjectOld.GetGeneration() != e.ObjectNew.GetGeneration()
		},
		GenericFunc: func(_ event.GenericEvent) bool {
			return false
		},
	}
}
```

### Integration with PodCliqueSet Controller

The PodCliqueSet controller is modified to use the backend during PodGang reconciliation:

```go
func (r *PodCliqueSetReconciler) reconcilePodGang(ctx context.Context, pcs *corev1alpha1.PodCliqueSet) error {
	// Get the configured backend
	backend, err := r.backendRegistry.GetActiveBackend()
	if err != nil {
		return fmt.Errorf("failed to get scheduler backend: %w", err)
	}
	
	podGang := &schedulerv1alpha1.PodGang{}
	err = r.Get(ctx, types.NamespacedName{
		Namespace: pcs.Namespace,
		Name:      pcs.Name,
	}, podGang)
	
	if apierrors.IsNotFound(err) {
		// Create new PodGang
		podGang = r.buildPodGang(pcs)
		
		// Call backend hook before creation
		if err := backend.OnPodGangCreate(ctx, r.Client, podGang, pcs); err != nil {
			return fmt.Errorf("backend OnPodGangCreate failed: %w", err)
		}
		
		if err := r.Create(ctx, podGang); err != nil {
			return fmt.Errorf("failed to create PodGang: %w", err)
		}
		
		return nil
	} else if err != nil {
		return fmt.Errorf("failed to get PodGang: %w", err)
	}
	
	// Update existing PodGang
	oldPodGang := podGang.DeepCopy()
	r.updatePodGang(podGang, pcs)
	
	// Call backend hook before update
	if err := backend.OnPodGangUpdate(ctx, r.Client, oldPodGang, podGang, pcs); err != nil {
		return fmt.Errorf("backend OnPodGangUpdate failed: %w", err)
	}
	
	if err := r.Update(ctx, podGang); err != nil {
		return fmt.Errorf("failed to update PodGang: %w", err)
	}
	
	return nil
}
```

### Pod Template Mutation

During pod creation, the backend mutates the pod specification:

```go
func (r *PodCliqueSetReconciler) createPod(ctx context.Context, podGroup *schedulerv1alpha1.PodGroup, podGang *schedulerv1alpha1.PodGang) error {
	pod := r.buildPodFromTemplate(podGroup)
	
	// Get the configured backend
	backend, err := r.backendRegistry.GetActiveBackend()
	if err != nil {
		return fmt.Errorf("failed to get scheduler backend: %w", err)
	}
	
	// Allow backend to mutate pod spec
	if err := backend.MutatePodSpec(ctx, &pod.Spec, podGroup, podGang); err != nil {
		return fmt.Errorf("backend MutatePodSpec failed: %w", err)
	}
	
	if err := r.Create(ctx, pod); err != nil {
		return fmt.Errorf("failed to create pod: %w", err)
	}
	
	return nil
}

### Monitoring

#### Metrics

The Scheduler Backend Framework will expose the following Prometheus metrics:

```go
// Backend initialization metrics
grove_scheduler_backend_initialization_duration_seconds
  Labels: backend_name, status (success/failure)
  
// Backend operation metrics  
grove_scheduler_backend_operation_duration_seconds
  Labels: backend_name, operation (create/update/delete/status_update/mutate)
  
grove_scheduler_backend_operation_total
  Labels: backend_name, operation, status (success/failure)
  
// Active backend info
grove_scheduler_backend_info
  Labels: backend_name, version
  Value: 1 (gauge indicating active backend)
```

#### Events

The operator will emit Kubernetes events for backend-related activities:

| Event Type | Reason | Message |
|------------|--------|---------|
| Normal | BackendInitialized | "Scheduler backend '%s' initialized successfully" |
| Warning | BackendInitializationFailed | "Failed to initialize scheduler backend '%s': %v" |
| Warning | BackendOperationFailed | "Backend operation '%s' failed for PodGang '%s': %v" |
| Normal | BackendSwitched | "Switched from backend '%s' to '%s'" |

#### Status Conditions

A new condition will be added to the Grove operator's status:

```go
// Condition: SchedulerBackendReady
// Status: True/False/Unknown
// Reason: BackendInitialized/BackendInitializationFailed/BackendNotConfigured
// Message: Detailed status of the backend initialization
```

#### Logging

Backend operations will be logged with structured logging:

```go
log.Info("Backend operation started",
    "backend", backendName,
    "operation", "OnPodGangCreate",
    "podgang", podGangName)
    
log.Error(err, "Backend operation failed",
    "backend", backendName,
    "operation", "OnPodGangCreate",
    "podgang", podGangName)
```

### Dependencies

#### Required Dependencies

1. **Kubernetes 1.24+**: The framework uses features introduced in Kubernetes 1.24.

2. **Controller Runtime v0.14+**: Required for client interfaces and controller patterns.

3. **Scheduler Implementation**: Each backend requires its corresponding scheduler to be deployed:
   - KAI Backend: [KAI Scheduler](https://github.com/NVIDIA/KAI-Scheduler) must be deployed
   - Default Backend (future): Standard Kubernetes scheduler with gang scheduling support

#### Optional Dependencies

1. **Monitoring Stack**: Prometheus and Grafana for metrics visualization (recommended for production).

2. **Backend-Specific CRDs**: Some backends may require additional CRDs to be installed:
   - KAI Backend: KAI PodGang CRD, Topology CRD (if TAS is enabled)
   - Future backends: Custom CRDs as defined by the scheduler implementation

#### Backend Registration

Backends must be compiled into the Grove operator binary. External/dynamic backend loading is not supported in the initial implementation. Backend packages should register themselves in their `init()` functions:

```go
package mybackend

import "github.com/ai-dynamo/grove/operator/internal/controller/backend"

func init() {
    backend.Register("mybackend", NewMyBackend)
}
```

To include a backend in the Grove operator build, import it in the main package:

```go
package main

import (
    _ "github.com/ai-dynamo/grove/operator/internal/controller/backend/kai"
    _ "github.com/ai-dynamo/grove/operator/internal/controller/backend/default"
    // Add new backends here
)
```

### Test Plan

#### Unit Tests

Unit tests will be implemented for all framework components:

**Backend Interface and Registry** (`operator/internal/controller/backend/`)
- Test backend registration (success, duplicate registration)
- Test backend retrieval (existing, non-existing)
- Test backend listing
- Test concurrent registration/retrieval

**KAI Backend Implementation** (`operator/internal/controller/backend/kai/`)
- Test initialization with various configurations
- Test PodGang lifecycle hooks (create, update, delete, status update)
- Test pod spec mutation
- Test configuration validation
- Test error handling and edge cases

**Controller Integration** (`operator/internal/controller/podcliqueset/`)
- Test PodGang reconciliation with backend hooks
- Test backend errors and failure handling
- Test backend switching scenarios
- Test pod creation with backend mutation

**OperatorConfiguration** (`operator/api/config/v1alpha1/`)
- Test configuration validation
- Test backend configuration parsing
- Test default values
- Test invalid backend names

#### Integration Tests

Integration tests will verify end-to-end workflows:

1. **Backend Initialization**
   - Verify operator starts with KAI backend configured
   - Verify backend initialization is called
   - Verify backend-specific resources are created
   - Verify operator fails to start with invalid backend configuration

2. **PodCliqueSet Lifecycle with KAI Backend**
   - Create PodCliqueSet and verify PodGang creation
   - Verify KAI backend hooks are called
   - Verify pod specs have correct scheduler name and annotations
   - Update PodCliqueSet and verify backend update hooks
   - Delete PodCliqueSet and verify cleanup

3. **Backend Failure Handling**
   - Test operator behavior when backend hooks return errors
   - Verify appropriate events and status conditions
   - Verify retry behavior

4. **Multiple PodCliqueSets**
   - Create multiple PodCliqueSets concurrently
   - Verify backend handles concurrent operations correctly
   - Verify no resource conflicts or race conditions

#### E2E Tests

End-to-end tests will verify real-world scenarios with actual schedulers:

1. **KAI Scheduler Integration**
   - Deploy Grove with KAI backend
   - Deploy KAI scheduler
   - Create PodCliqueSet with gang scheduling requirements
   - Verify pods are gang-scheduled by KAI
   - Verify topology constraints are honored (if TAS is enabled)

2. **Workload Scheduling Scenarios**
   - Test simple gang scheduling (all pods scheduled together)
   - Test gang scheduling with topology constraints
   - Test scaling PodCliqueSet replicas
   - Test PodCliqueScalingGroup scaling

3. **Failure and Recovery**
   - Test scheduler failure and recovery
   - Test operator restart with existing PodCliqueSets
   - Test pod failure within a gang
   - Test node failure and rescheduling

#### Test Coverage Goals

- Unit test coverage: ≥ 80% for all backend framework code
- Integration test coverage: All major workflows covered
- E2E test coverage: All supported scheduling scenarios covered

#### Test Issue Tracking

A dedicated issue will be created to track test implementation:
- Issue: "Implement tests for GREP-245: Scheduler Backend Framework"
- The issue will contain detailed test scenarios and acceptance criteria

### Graduation Criteria

The Scheduler Backend Framework will follow a staged rollout approach:

#### Alpha (v0.1.0)

**Goals:**
- Core backend interface defined and implemented
- Backend registry functional
- KAI backend fully implemented as reference
- Basic operator configuration support
- Unit tests achieving ≥80% coverage

**Success Criteria:**
- KAI backend works with existing functionality (no regressions)
- Operator can be configured with backend selection
- Backend hooks are called correctly during PodGang lifecycle
- Documentation covers backend interface and KAI implementation

**Known Limitations:**
- Only KAI backend supported
- No support for backend switching without operator restart
- Limited observability and metrics
- Backend API may still evolve

#### Beta (v0.2.0)

**Goals:**
- Backend interface stabilized (no breaking changes expected)
- Comprehensive metrics and monitoring
- Integration tests for all backend operations
- E2E tests with KAI scheduler
- Documentation for third-party backend development
- At least one additional backend implemented (e.g., default scheduler support)

**Success Criteria:**
- No critical bugs reported in alpha
- Backend switching via operator restart works reliably
- Metrics provide sufficient observability into backend operations
- Third-party developers can implement backends using documentation
- Performance benchmarks show no degradation compared to pre-framework implementation

**Known Limitations:**
- Runtime backend switching not supported
- Limited support for custom scheduler features beyond gang scheduling

#### GA (v1.0.0)

**Goals:**
- Backend interface is stable and versioned
- Multiple production deployments using the framework
- Comprehensive documentation and examples
- All tests passing consistently
- Performance optimizations complete
- Support for at least 2-3 different scheduler backends

**Success Criteria:**
- Framework has been used in production for at least 2 releases
- No major bugs or design flaws identified
- Backend API is backward compatible within v1
- Clear migration path from pre-framework Grove versions
- Community adoption and positive feedback

**Promotion Requirements:**
- At least 3 months in beta
- Production usage by multiple organizations
- All beta success criteria met
- Security review completed
- API review completed and approved

#### Future Enhancements (Post-GA)

- Dynamic backend loading (plugin system)
- Runtime backend switching for new workloads
- Multi-backend support per cluster
- Advanced backend capabilities (custom metrics, health checks, etc.)
- Backend versioning and compatibility matrix

## Implementation History

Major milestones in the lifecycle of this GREP:

- **2026-01-26**: Initial GREP proposal created and submitted for review
- **TBD**: GREP proposal accepted and merged
- **TBD**: Implementation started (Alpha phase)
- **TBD**: Alpha release (v0.1.0)
- **TBD**: Beta release (v0.2.0)
- **TBD**: GA release (v1.0.0)

## Alternatives

### Alternative 1: Direct Scheduler Integration (Status Quo)

**Description**: Continue the current approach where scheduler-specific logic is embedded directly in Grove's controllers without abstraction.

**Pros**:
- No additional abstraction layer overhead
- Direct control over scheduler integration
- Simpler codebase for single-scheduler scenarios

**Cons**:
- High maintenance burden as schedulers evolve
- Adding new schedulers requires invasive changes to Grove core
- Difficult for third-party schedulers to integrate
- Code becomes increasingly complex with each scheduler addition
- **Rejected**: Does not scale as Grove adoption grows across different scheduler ecosystems

### Alternative 2: Webhook-Based Mutating Admission

**Description**: Use Kubernetes admission webhooks to mutate PodGang and Pod resources based on annotations indicating the target scheduler.

**Pros**:
- Complete separation from Grove operator code
- Dynamic registration without operator restarts
- Could support multiple schedulers simultaneously

**Cons**:
- Adds operational complexity (webhook certificates, availability)
- Performance overhead of webhook calls
- Debugging and troubleshooting is more difficult
- State management becomes complex
- **Rejected**: The additional operational complexity and performance overhead outweigh the benefits for this use case

### Alternative 3: CRD-Based Backend Configuration

**Description**: Define scheduler backends as CRDs that users create and reference from PodCliqueSets.

**Pros**:
- Highly flexible and declarative
- Supports multiple backends per cluster
- Users can customize per-workload

**Cons**:
- Significantly more complex API surface
- User experience degradation (more resources to manage)
- Validation becomes challenging
- Unclear ownership and lifecycle management
- **Rejected**: Adds too much complexity for the primary use case of single-scheduler clusters

### Alternative 4: Pluggable Backend with Dynamic Loading

**Description**: Support loading backend implementations from external binaries or shared libraries at runtime.

**Pros**:
- Backends can be developed and deployed independently
- No need to recompile Grove for new backends
- Truly pluggable architecture

**Cons**:
- Security risks (loading untrusted code)
- Version compatibility challenges
- Platform-specific build requirements
- Deployment and upgrade complexity
- **Rejected**: While desirable for future enhancements, this adds too much complexity for the initial implementation. Can be revisited post-GA.

### Alternative 5: Scheduler-Side Implementation

**Description**: Push all integration logic to the scheduler side, with Grove only managing PodGang resources in a scheduler-agnostic way.

**Pros**:
- Grove remains completely scheduler-agnostic
- Schedulers have full control over their integration

**Cons**:
- Requires more invasive changes to each scheduler
- Duplicated logic across scheduler implementations
- Grove loses ability to provide consistent user experience
- Difficult to maintain common functionality (e.g., topology translation)
- **Rejected**: Places too much burden on scheduler implementations and loses the value Grove provides in workload management

### Selected Approach: Interface-Based Backend Framework

The selected approach (described in this GREP) provides the best balance of:
- **Extensibility**: Easy to add new backends
- **Maintainability**: Clean separation of concerns
- **User Experience**: Consistent API across schedulers
- **Simplicity**: Straightforward implementation without over-engineering
- **Evolution Path**: Can evolve toward more advanced architectures (dynamic loading) in the future

## Appendix

### Related Documentation

- **GREP-244: Topology Aware Scheduling**: Provides context on topology constraints that backends must support
- **PodGang API Specification**: Core scheduler API that backends interact with
- **PodCliqueSet API Documentation**: User-facing API that drives backend operations

### References

- [Kubernetes Scheduler Framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)
- [KAI Scheduler](https://github.com/NVIDIA/KAI-Scheduler)
- [Gang Scheduling in Kubernetes](https://github.com/kubernetes-sigs/scheduler-plugins/tree/master/pkg/coscheduling)
- [KEP Template](https://github.com/kubernetes/enhancements/blob/f90055d254c356b2c038a1bdf4610bf4acd8d7be/keps/NNNN-kep-template/README.md)

### Example Backend Implementation Template

For developers implementing new scheduler backends, here's a minimal template:

```go

// Backend implements the SchedulerBackend interface for your custom scheduler.
type Backend struct {
	client        client.Client
	scheme        *runtime.Scheme
	eventRecorder record.EventRecorder
	schedulerName string
}

// New creates a new backend instance.
func New(cl client.Client, scheme *runtime.Scheme, eventRecorder record.EventRecorder, schedulerName string) *Backend {
	return &Backend{
		client:        cl,
		scheme:        scheme,
		eventRecorder: eventRecorder,
		schedulerName: schedulerName,
	}
}

// Name returns the backend name.
func (b *Backend) Name() string {
	return BackendName
}

// Init initializes the backend.
// Use this to create global resources, validate configuration, etc.
func (b *Backend) Init() error {
	// Example: Create global scheduler-specific resources
	// Example: Validate scheduler is available
	return nil
}

// SyncPodGang synchronizes PodGang to scheduler-specific resources.
// This is called by the Backend Controller when PodGang is created or spec changes.
func (b *Backend) SyncPodGang(ctx context.Context, podGang *groveschedulerv1alpha1.PodGang) error {
	// Example: Create/update scheduler-specific CRs (PodGroup, Workload, etc.)
	// Use owner references to link resources to PodGang for automatic cleanup
	return nil
}

// OnPodGangDelete cleans up scheduler-specific resources.
// If using owner references, this can be a no-op.
func (b *Backend) OnPodGangDelete(ctx context.Context, podGang *groveschedulerv1alpha1.PodGang) error {
	// Example: Delete scheduler-specific resources if not using owner references
	return nil
}

// PreparePod adds scheduler-specific configuration to pods.
// Called during pod creation in PodClique controller.
func (b *Backend) PreparePod(pod *corev1.Pod) {
	// Set scheduler name
	pod.Spec.SchedulerName = b.schedulerName
	
	// Add annotations for your scheduler
	if pod.Annotations == nil {
		pod.Annotations = make(map[string]string)
	}
	pod.Annotations["mybackend.io/example"] = "value"
	
	// Add labels if needed
	if pod.Labels == nil {
		pod.Labels = make(map[string]string)
	}
	pod.Labels["mybackend.io/managed"] = "true"
}
```

**To add your backend to Grove:**

1. Create a new package: `operator/internal/schedulerbackend/mybackend/`
2. Add your backend to the manager's switch statement in `operator/internal/schedulerbackend/manager.go`:
   ```go
   case "mybackend-scheduler":
       globalBackend = mybackend.New(client, scheme, eventRecorder, schedulerName)
   ```
3. Import your backend package in `operator/internal/controller/register.go`
4. Rebuild the Grove operator

### Migration Guide

#### Migrating Existing KAI Deployments

For existing Grove deployments using KAI scheduler, the migration is seamless:

**Pre-Framework Behavior:**
- Pods were created with KAI scheduler name hardcoded
- PodGang was created after pods existed

**Post-Framework Behavior:**
- PodGang is created first (with empty PodReferences)
- Pods are created with scheduler name from configuration
- Backend handles scheduler-specific logic

**Migration Steps:**

1. **Update OperatorConfiguration** (Helm values or ConfigMap):
   ```yaml
   # Add this field to your OperatorConfiguration
   schedulerName: "kai-scheduler"
   ```

2. **Helm Chart Update**:
   ```yaml
   # values.yaml
   config:
     schedulerName: "kai-scheduler"  # or "default-scheduler"
   ```

3. **Rolling Update**: Upgrade Grove operator to the version with backend framework

4. **Validation**:
   - Verify existing PodCliqueSets continue to work
   - Check that new pods have correct schedulerName set
   - Verify PodGang resources are created with `Initialized` condition

5. **Monitor**:
   - Check logs for backend initialization messages
   - Verify no scheduling delays or failures
   - Monitor PodGang status conditions

**Backward Compatibility:**
- If `schedulerName` is not specified, it defaults to `"default-scheduler"`
- Existing KAI deployments should explicitly set `schedulerName: "kai-scheduler"`
- No changes required to PodCliqueSet specs

#### Switching Scheduler Backends

To switch from one scheduler to another (e.g., KAI to default scheduler):

1. **Update Configuration**:
   ```yaml
   schedulerName: "default-scheduler"  # Changed from "kai-scheduler"
   ```

2. **Restart Operator**: Rolling restart picks up new configuration

3. **New Workloads**: Will use the new scheduler automatically

4. **Existing Workloads**: Continue using their original scheduler until redeployed

**Note**: Switching schedulers does not affect running workloads. Only new PodCliqueSets will use the new scheduler.

#### Adding a New Scheduler Backend

For developers adding support for a new scheduler:

1. **Create Backend Package**: 
   ```bash
   mkdir operator/internal/schedulerbackend/myscheduler
   ```

2. **Implement Interface**: Create `backend.go` implementing `SchedulerBackend` interface

3. **Update Manager**: Add your backend to `operator/internal/schedulerbackend/manager.go`:
   ```go
   case "myscheduler-name":
       globalBackend = myscheduler.New(client, scheme, eventRecorder, schedulerName)
   ```

4. **Import Backend**: Add import in `operator/internal/controller/register.go`

5. **Write Tests**: Implement unit and integration tests

6. **Document**: Add usage documentation and examples

7. **Build and Deploy**: Recompile Grove operator with your backend included

**Example PR Structure:**
- `operator/internal/schedulerbackend/myscheduler/backend.go` - Backend implementation
- `operator/internal/schedulerbackend/myscheduler/backend_test.go` - Unit tests
- `operator/internal/schedulerbackend/manager.go` - Add case for your scheduler
- `docs/schedulers/myscheduler.md` - Usage documentation

### Glossary

- **Backend**: An implementation of the SchedulerBackend interface that integrates Grove with a specific scheduler
- **Backend Manager**: A singleton manager that initializes and provides access to the active scheduler backend
- **Backend Controller**: A dedicated controller that watches PodGang resources and invokes backend `SyncPodGang` and `OnPodGangDelete` methods
- **Scheduler Backend Framework**: The overall architecture and interfaces enabling pluggable scheduler support in Grove
- **Active Backend**: The currently configured and initialized scheduler backend in a Grove deployment
- **PodGang Initialized Condition**: A status condition indicating that PodGang has been populated with pod references and pods can proceed with scheduling
- **PreparePod**: The backend method called during pod creation to inject scheduler-specific configuration (schedulerName, annotations, etc.)
- **SyncPodGang**: The backend method called to create/update scheduler-specific custom resources in response to PodGang changes

> NOTE: This GREP template has been inspired by [KEP Template](https://github.com/kubernetes/enhancements/blob/f90055d254c356b2c038a1bdf4610bf4acd8d7be/keps/NNNN-kep-template/README.md).

